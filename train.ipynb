{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data:\n",
      "               label     band1_m     band2_m     band3_m     band4_m\n",
      "597   Metálica verde  117.844242  135.537576  130.729697  115.951515\n",
      "1237   Metálica rojo  156.802271  110.879684  100.413330  128.123525\n",
      "629     Fibrocemento  103.803045  115.178732  109.697703  148.398902\n",
      "214            Otros  169.044655  175.937370  160.107663  131.886093\n",
      "1195    Fibrocemento  161.451156  166.530931  145.755780  141.268590\n",
      "value counts:\n",
      "Metálica verde     237\n",
      "Metálica blanco    210\n",
      "Otros              203\n",
      "Fibrocemento       202\n",
      "Metálica rojo      200\n",
      "Tejado rojo        196\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sklearn\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "\n",
    "# load dataset\n",
    "fn = 'data.csv'\n",
    "import pandas as pd\n",
    "col_names = ['label', 'band1_m', 'band2_m', 'band3_m', 'band4_m']\n",
    "feat_cols = col_names[1:]\n",
    "df = pd.read_table( fn, delimiter=',', header=None, names=col_names )\n",
    "df = sklearn.utils.shuffle( df )\n",
    "\n",
    "# \n",
    "print( 'data:' )\n",
    "print( df.head() )\n",
    "print( 'value counts:' )\n",
    "print( df.label.value_counts() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "X = df[ feat_cols ].values\n",
    "y = df[ col_names[0] ].values\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit( y )\n",
    "y = le.transform( y )\n",
    "print( 'classes:', le.classes_ )\n",
    "#  lb = preprocessing.LabelBinarizer()\n",
    "#  lb.fit( y )\n",
    "#  y = lb.transform( y )\n",
    "#  print( 'classes:', lb.classes_ )\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2 )\n",
    "\n",
    "\n",
    "\n",
    "# normalization\n",
    "scaler = preprocessing.StandardScaler().fit( X_train )\n",
    "X_scaled = scaler.transform( X )\n",
    "X_train_scaled = scaler.transform( X_train )\n",
    "X_test_scaled  = scaler.transform( X_test )\n",
    "\n",
    "\n",
    "\n",
    "# LDA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "lda = LDA( n_components=4 )\n",
    "lda.fit( X_train_scaled, y_train )\n",
    "\n",
    "\n",
    "\n",
    "# visualize train set dicriminant components\n",
    "X_lda = lda.transform( X_train_scaled )\n",
    "#\n",
    "x_DC1 = X_lda[:,0]\n",
    "x_DC2 = X_lda[:,1]\n",
    "for class_ in le.classes_ :\n",
    "    I = y_train == le.transform([ class_ ])\n",
    "    color = np.random.rand(1,3)\n",
    "    color = tuple( color[0,:] )\n",
    "    plt.plot( x_DC1[I], x_DC2[I], linestyle='', marker='o', color=color, alpha=.5 )\n",
    "plt.legend( le.classes_ )\n",
    "plt.show()\n",
    "\n",
    "#  #\n",
    "#  from sklearn.ensemble import RandomForestClassifier\n",
    "#  classifier = RandomForestClassifier( max_depth=2, random_state=0 )\n",
    "#  classifier.fit( X_scaled, y )\n",
    "#  y_pred = classifier.predict( X_scaled )\n",
    "#  from sklearn.metrics import accuracy_score\n",
    "#  print('Accuracy {}%'.format( accuracy_score(y, y_pred) ))\n",
    "\n",
    "#  from sklearn import linear_model\n",
    "#  lm = linear_model.LinearRegression()\n",
    "#  model = lm.fit( X_train, y_train )\n",
    "#  predictions = lm.predict( X_test )\n",
    "\n",
    "\n",
    "\n",
    "# visualize test set dicriminant components\n",
    "X_lda = lda.transform( X_test_scaled )\n",
    "#\n",
    "x_DC1 = X_lda[:,0]\n",
    "x_DC2 = X_lda[:,1]\n",
    "for class_ in le.classes_ :\n",
    "    I = y_test == le.transform([ class_ ])\n",
    "    color = np.random.rand(1,3)\n",
    "    color = tuple( color[0,:] )\n",
    "    plt.plot( x_DC1[I], x_DC2[I], linestyle='', marker='o', color=color, alpha=.5 )\n",
    "plt.legend( le.classes_ )\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "#  from sklearn.svm import SVC\n",
    "#  clf = SVC( gamma='scale', decision_function_shape='ovo' )\n",
    "#  clf.fit( X_train_scaled, y_train ) \n",
    "#  #\n",
    "#  y_train_pred = clf.predict( X_train_scaled )\n",
    "#  y_test_pred  = clf.predict( X_test_scaled )\n",
    "#  #\n",
    "#  from sklearn.metrics import accuracy_score\n",
    "#  print('Train accuracy {}%'.format( accuracy_score(y_train, y_train_pred) ))\n",
    "#  print('Test accuracy {}%'.format( accuracy_score(y_test, y_test_pred) ))\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "clf = SVC( gamma='scale', decision_function_shape='ovo' )\n",
    "X_train_lda = lda.transform( X_train_scaled )\n",
    "X_test_lda  = lda.transform( X_test_scaled )\n",
    "clf.fit( X_train_lda, y_train ) \n",
    "#\n",
    "y_train_pred = clf.predict( X_train_lda )\n",
    "y_test_pred  = clf.predict( X_test_lda )\n",
    "#\n",
    "from sklearn.metrics import accuracy_score\n",
    "print('Train accuracy {}%'.format( accuracy_score(y_train, y_train_pred) ))\n",
    "print('Test accuracy {}%'.format( accuracy_score(y_test, y_test_pred) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.pairplot( df, hue='label' )\n",
    "plt.show()\n",
    "\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA( n_components=len(feat_cols) )\n",
    "pca_result = pca.fit_transform( X_scaled )\n",
    "plt.plot( range(4), pca.explained_variance_ratio_ )\n",
    "plt.plot( range(4), np.cumsum(pca.explained_variance_ratio_) )\n",
    "plt.title( 'Component-wise and Cumulative Explained Variance' )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from osgeo import gdal\n",
    "from osgeo import ogr\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "\n",
    "# load shp\n",
    "print( 'loading shp..' )\n",
    "shp_fn = 'data2/shp/Edif_Clases.shp'\n",
    "shp = ogr.Open( shp_fn , 0 ) # 0 means read-only. 1 means writeable.\n",
    "layer = shp.GetLayer()\n",
    "print( 'len(layer):', len(layer) )\n",
    "#\n",
    "data = []\n",
    "for idx, feature in enumerate(layer) :\n",
    "    envelope = feature.GetGeometryRef().GetEnvelope()\n",
    "    xmin, xmax, ymin, ymax = envelope\n",
    "    #  roi_bbox = xmin, ymax, xmax, ymin\n",
    "    # extend roi\n",
    "    margin = 14\n",
    "    roi_bbox = xmin-margin, ymax+margin, xmax+margin, ymin-margin\n",
    "\n",
    "    #  # dbg\n",
    "    #  idxs = find_intersecting_rasters( rasters_geom, roi_bbox )\n",
    "    #  if len(idxs) < 1 :\n",
    "    #      assert(False), 'roi does not intersect any raster'\n",
    "\n",
    "    #\n",
    "    subraster = get_roi( filenames, rasters_geom, roi_bbox )\n",
    "\n",
    "    # subraster_mask\n",
    "    shp_idx = ogr.GetDriverByName('Memory').CreateDataSource('')\n",
    "    source_layer = shp_idx.CreateLayer('states_extent')\n",
    "    source_layer.CreateFeature( feature )\n",
    "    subraster_mask = draw_mask( subraster, source_layer )\n",
    "\n",
    "    #\n",
    "    img = raster2img( subraster )\n",
    "    mask = raster2img( subraster_mask )\n",
    "\n",
    "    # \n",
    "    kernel = np.ones( (1+9,1+9), np.uint8 )\n",
    "    #  mask2 = cv2.dilate( mask, kernel, iterations=1 )\n",
    "    mask1 = mask\n",
    "    mask2 = cv2.erode( mask, kernel, iterations=1 )\n",
    "    img1 = np.array( img )\n",
    "    img2 = np.array( img )\n",
    "    I1 = mask1 == 0\n",
    "    I2 = mask2 == 0\n",
    "    for k in range(img.shape[2]) :\n",
    "        img1[ I1, k ] = 0\n",
    "        img2[ I2, k ] = 0\n",
    "    # extract features in the mask\n",
    "    img3 = np.array( img )\n",
    "    #  img3 = img3.reshape((-1,3))\n",
    "    color_m   = img3[ mask1 != 0 ].mean(axis=0)\n",
    "    color_std = img3[ mask1 != 0 ].std(axis=0)\n",
    "    print( feature['TIPO_CUBIE'], 'color:', color_m, color_std )\n",
    "    data.append( (feature['TIPO_CUBIE'], color_m) )\n",
    "\n",
    "\n",
    "print( 'data' )\n",
    "for label, color in data :\n",
    "    print( '{},{}'.format( label, ','.join(list(map(str,color))) ) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "def get_dataset() :\n",
    "    # load .shp\n",
    "    \n",
    "    X_files, y = ds\n",
    "    X = []\n",
    "    for fn in X_files :\n",
    "        img = cv2.imread( fn )[..., ::-1]\n",
    "        img = np.float32( img )\n",
    "#         img = img[...,0]/255 + img[...,1]/255 + img[...,2]/255\\n\",\n",
    "        X.append( img )\n",
    "    X = np.stack( X, axis=0 )\n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pix_sz() :\n",
    "    raise 'TODO'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  def get_img( size ) : # for Mask R-CNN (a layer in the img defines the labels)\n",
    "#      pass\n",
    "\n",
    "def get_img( roi_poly, bbox ) : # for classifiers (a layer in the img defines the ROI)\n",
    "    raise 'TODO'\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_roi_bbox( poly, pix_sz, shift ) :\n",
    "    raise 'TODO'\n",
    "    return bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_shape (416, 416, 3)\n",
      "Layer 1: (?, 414, 414, 16)\n",
      "Layer 2: (?, 205, 205, 24)\n",
      "Layer 3: (?, 100, 100, 36)\n"
     ]
    }
   ],
   "source": [
    "def create_classifier_model( input_shape, n_classes ) :\n",
    "\n",
    "    from keras.models import Model\n",
    "    from keras.layers import Cropping2D, Lambda, Reshape\n",
    "    from keras.layers import Conv2D, MaxPooling2D\n",
    "    from keras.layers import Input, Dense, ELU, Activation, Flatten\n",
    "    from keras.layers import Dropout, BatchNormalization\n",
    "\n",
    "    from keras import backend as K\n",
    "    print('input_shape', input_shape)\n",
    "    input_ = Input( shape=input_shape )\n",
    "#     hidden = Lambda( lambda x: x[:,:,:,0]/3 + x[:,:,:,1]/3 + x[:,:,:,2]/3 )( input_ )\\n\",\n",
    "#     hidden = Reshape( (*input_shape[:-1], 1) )( hidden )\\n\",\n",
    "    hidden = input_\n",
    "    hidden = BatchNormalization()( hidden )\n",
    "#     hidden = Cropping2D( cropping=((0,0),(0,0)) )( input_ )\n",
    "    with K.name_scope('Layer1') :\n",
    "        hidden = Conv2D( 16, 3 )( hidden )\n",
    "        print( 'Layer 1:', hidden.shape )\n",
    "        hidden = MaxPooling2D( pool_size=(2, 2) )( hidden )\n",
    "        hidden = ELU()( hidden )\n",
    "    with K.name_scope('Layer2') :\n",
    "        hidden = Conv2D(24, 3)( hidden )\n",
    "        print( 'Layer 2:', hidden.shape )\n",
    "        hidden = MaxPooling2D(pool_size=(2, 2))( hidden )\n",
    "        hidden = ELU()( hidden )\n",
    "    with K.name_scope('Layer3') :\n",
    "        hidden = Conv2D(36, 3)( hidden )\n",
    "        print( 'Layer 3:', hidden.shape )\n",
    "        hidden = MaxPooling2D(pool_size=(2, 2))( hidden )\n",
    "        hidden = ELU()( hidden )\n",
    "    hidden = Dropout(.5)( hidden )\n",
    "    hidden = Flatten()( hidden )\n",
    "    with K.name_scope('Layer4') :\n",
    "        hidden = Dense(512)( hidden )\n",
    "        hidden = ELU()( hidden )\n",
    "    #  with K.name_scope('Layer5'):\\n\",\n",
    "    #      model.add(Dense(512))\\n\",\n",
    "    #      #  model.add(BatchNormalization())\\n\",\n",
    "    #      #  model.add(Dropout(.3))\\n\",\n",
    "    #      model.add(ELU()) # model.add(Activation('relu'))\\n\",\n",
    "    with K.name_scope('Output') :\n",
    "        #  model.add(Dropout(.3))\n",
    "        hidden = Dense(n_classes)( hidden )\n",
    "        predictions = Activation('softmax')( hidden )\n",
    "    model = Model(inputs=input_, outputs=predictions)\n",
    "    return model\n",
    "# dbg\n",
    "input_shape = (416,416,3)\n",
    "n_classes = 4\n",
    "model = create_classifier_model( input_shape, n_classes )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------\n",
      "[DBG] DataGenerator::len()\n",
      "[DBG] on_epoch_begin()!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "-- batch (size=2)\n",
      "(2, 416, 416, 3) (2, 6)\n",
      "<class 'numpy.ndarray'> <class 'numpy.ndarray'>\n",
      "-- batch (size=2)\n",
      "(2, 416, 416, 3) (2, 6)\n",
      "<class 'numpy.ndarray'> <class 'numpy.ndarray'>\n",
      "-- batch (size=2)\n",
      "(2, 416, 416, 3) (2, 6)\n",
      "<class 'numpy.ndarray'> <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# See: https://stanford.edu/~shervine/blog/keras-how-to-generate-data-on-the-fly\n",
    "# MOCKS\n",
    "import numpy as np\n",
    "import keras\n",
    "def get_roi_bbox( poly, pix_sz, sift ) :\n",
    "    bbox = (0,1,1,0)\n",
    "    return bbox\n",
    "def get_img( poly, bbox ) :\n",
    "    input_shape = 416, 416, 3\n",
    "    #  img = np.zeros( (416,416,4), np.uint8 )\n",
    "    img = (255 * np.random.rand( *input_shape )).astype('uint8')\n",
    "    return img\n",
    "#\n",
    "class DataGenerator(keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    \n",
    "    def __init__(self, polys, labels, n_classes, batch_size=32 ):\n",
    "        self.labels = labels\n",
    "        self.polys = polys\n",
    "        self.n_classes = n_classes\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        print('[DBG] DataGenerator::len()')\n",
    "        self.on_epoch_begin()\n",
    "        return int(np.floor(len(self.labels) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[ index*self.batch_size : (index+1)*self.batch_size ]\n",
    "        # Generate data\n",
    "        X = []\n",
    "        y = []\n",
    "        for k in range( self.batch_size ) :\n",
    "            k2 = indexes[k]\n",
    "            poly = self.polys[ k2 ]\n",
    "            sift = .25\n",
    "            pix_sz = .5\n",
    "            bbox = get_roi_bbox( poly, pix_sz, sift )\n",
    "            img = get_img( poly, bbox )\n",
    "            X.append( img )\n",
    "            y.append( self.labels[ k2 ] )\n",
    "        X = np.stack( X, axis=0 )            \n",
    "        Y = keras.utils.to_categorical( y, num_classes=self.n_classes )\n",
    "        #\n",
    "        return X, Y\n",
    "\n",
    "    def on_epoch_begin(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len( self.labels ))\n",
    "        np.random.shuffle( self.indexes )\n",
    "        print('[DBG] on_epoch_begin()!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!')\n",
    "      \n",
    "    \n",
    "# def create_train_generator( x_poly_train, y_train, n_classes, input_shape, batch_size=16 ) :\n",
    "#     n = len( y_train )\n",
    "#     h_pix, w_pix = input_shape[:2]\n",
    "#     pix_sz = get_pix_sz()\n",
    "#     h_wc = pix_sz * h # wc (world coords)\n",
    "#     w_wc = pix_sz * w # wc (world coords)\n",
    "#     y_delta = h_world * (np.random.rand() - .5) * .5\n",
    "#     x_delta = w_world * (np.random.rand() - .5) * .5\n",
    "#     #\n",
    "    \n",
    "#     train_datagen = ImageDataGenerator(\n",
    "#             rescale=1,\n",
    "#             shear_range=0,\n",
    "#             zoom_range=0,\n",
    "#             horizontal_flip=True)\n",
    "\n",
    "#     train_generator = TODO( train_datagen, batch_size=32 )\n",
    "    \n",
    "#     for x_batch, y_batch in datagen.flow( x_train, y_train ):\n",
    "#         model.fit(x_batch, y_batch)\n",
    "#         batches += 1\n",
    "#         if batches >= len(x_train) / 32:\n",
    "#             # we need to break the loop by hand because\n",
    "#             # the generator loops indefinitely\n",
    "#             break    \n",
    "    \n",
    "#     #\n",
    "#     for poly in x_poly_train :\n",
    "#         centroid_x, centroid_y = TODO( poly )\n",
    "#         x_min = centroid_x - w_wc/2 + x_delta\n",
    "#         x_max = centroid_x + w_wc/2 + x_delta\n",
    "#         y_min = centroid_y - h_wc/2 + y_delta\n",
    "#         y_max = centroid_y + h_wc/2 + y_delta\n",
    "#         bbox = x_min, y_max, x_max, y_min\n",
    "#         img = get_img( poly, bbox )\n",
    "#     #\n",
    "#     raise 'TODO'\n",
    "#     yield img, \n",
    "    \n",
    "    \n",
    "polys = [ [(0,0)], [(0,1)], [(1,1)], [(1,0)], [(0,0)], [(5,5)] ]\n",
    "labels = [ k for k in range(6) ]\n",
    "n_classes = 6\n",
    "batch_size=2\n",
    "datagenerator = DataGenerator( polys, labels, n_classes, batch_size=batch_size )\n",
    "print('-------')\n",
    "for data in datagenerator :\n",
    "    print( '-- batch (size={})'.format(batch_size) )\n",
    "    print( data[0].shape, data[1].shape )\n",
    "    print( type(data[0]), type(data[1]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(x_poly_train): 800\n",
      "len(x_poly_test): 200\n",
      "len(y_train): 800\n",
      "len(y_test): 200\n",
      "n_classes: 3\n",
      "input_shape (416, 416, 3)\n",
      "Layer 1: (?, 414, 414, 16)\n",
      "Layer 2: (?, 205, 205, 24)\n",
      "Layer 3: (?, 100, 100, 36)\n",
      "[DBG] DataGenerator::len()\n",
      "[DBG] on_epoch_begin()!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "[DBG] DataGenerator::len()[DBG] DataGenerator::len()Epoch 1/30\n",
      "\n",
      "[DBG] on_epoch_begin()!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "\n",
      "[DBG] on_epoch_begin()!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "1/1 [==============================] - 3s 3s/step - loss: 1.8635 - acc: 0.1250 - val_loss: 10.0738 - val_acc: 0.3750\n",
      "Epoch 2/30\n",
      "1/1 [==============================] - 0s 122ms/step - loss: 15.1107 - acc: 0.0625 - val_loss: 10.0738 - val_acc: 0.3750\n",
      "Epoch 3/30\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 4/30\n",
      "1/1 [==============================] - 0s 122ms/step - loss: 10.0738 - acc: 0.3750 - val_loss: 11.0812 - val_acc: 0.3125\n",
      "Epoch 5/30\n",
      "1/1 [==============================] - 0s 115ms/step - loss: 12.0886 - acc: 0.2500 - val_loss: 12.0886 - val_acc: 0.2500\n",
      "Epoch 6/30\n",
      "1/1 [==============================] - 0s 120ms/step - loss: 14.1033 - acc: 0.1250 - val_loss: 12.0886 - val_acc: 0.2500\n",
      "Epoch 7/30\n",
      "1/1 [==============================] - 0s 121ms/step - loss: 12.0886 - acc: 0.2500 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 8/30\n",
      "1/1 [==============================] - 0s 120ms/step - loss: 11.0812 - acc: 0.3125 - val_loss: 10.0738 - val_acc: 0.3750\n",
      "Epoch 9/30\n",
      "1/1 [==============================] - 0s 117ms/step - loss: 10.0738 - acc: 0.3750 - val_loss: 9.0664 - val_acc: 0.4375\n",
      "Epoch 10/30\n",
      "1/1 [==============================] - 0s 116ms/step - loss: 10.0738 - acc: 0.3750 - val_loss: 9.0664 - val_acc: 0.4375\n",
      "Epoch 11/30\n",
      "1/1 [==============================] - 0s 119ms/step - loss: 12.0886 - acc: 0.2500 - val_loss: 9.0664 - val_acc: 0.4375\n",
      "Epoch 12/30\n",
      "1/1 [==============================] - 0s 122ms/step - loss: 8.0590 - acc: 0.5000 - val_loss: 9.0664 - val_acc: 0.4375\n",
      "Epoch 13/30\n",
      "1/1 [==============================] - 0s 173ms/step - loss: 11.0812 - acc: 0.3125 - val_loss: 10.0738 - val_acc: 0.3750\n",
      "Epoch 14/30\n",
      "1/1 [==============================] - 0s 150ms/step - loss: 9.0664 - acc: 0.4375 - val_loss: 10.0738 - val_acc: 0.3750\n",
      "Epoch 15/30\n",
      "1/1 [==============================] - 0s 156ms/step - loss: 13.0960 - acc: 0.1875 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 16/30\n",
      "1/1 [==============================] - 0s 164ms/step - loss: 12.0886 - acc: 0.2500 - val_loss: 11.0812 - val_acc: 0.3125\n",
      "Epoch 17/30\n",
      "1/1 [==============================] - 0s 160ms/step - loss: 9.0664 - acc: 0.4375 - val_loss: 12.0886 - val_acc: 0.2500\n",
      "Epoch 18/30\n",
      "1/1 [==============================] - 0s 163ms/step - loss: 11.0812 - acc: 0.3125 - val_loss: 12.0886 - val_acc: 0.2500\n",
      "Epoch 19/30\n",
      "1/1 [==============================] - 0s 159ms/step - loss: 12.0886 - acc: 0.2500 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 20/30\n",
      "1/1 [==============================] - 0s 158ms/step - loss: 12.0886 - acc: 0.2500 - val_loss: 10.0738 - val_acc: 0.3750\n",
      "Epoch 21/30\n",
      "1/1 [==============================] - 0s 168ms/step - loss: 8.0590 - acc: 0.5000 - val_loss: 9.0664 - val_acc: 0.4375\n",
      "Epoch 22/30\n",
      "1/1 [==============================] - 0s 154ms/step - loss: 9.0664 - acc: 0.4375 - val_loss: 9.0664 - val_acc: 0.4375\n",
      "Epoch 23/30\n",
      "1/1 [==============================] - 0s 150ms/step - loss: 9.0664 - acc: 0.4375 - val_loss: 9.0664 - val_acc: 0.4375\n",
      "Epoch 24/30\n",
      "1/1 [==============================] - 0s 157ms/step - loss: 11.0812 - acc: 0.3125 - val_loss: 9.0664 - val_acc: 0.4375\n",
      "Epoch 25/30\n",
      "1/1 [==============================] - 0s 204ms/step - loss: 10.0738 - acc: 0.3750 - val_loss: 10.0738 - val_acc: 0.3750\n",
      "Epoch 26/30\n",
      "1/1 [==============================] - 0s 145ms/step - loss: 10.0738 - acc: 0.3750 - val_loss: 10.0738 - val_acc: 0.3750\n",
      "Epoch 27/30\n",
      "1/1 [==============================] - 0s 165ms/step - loss: 12.0886 - acc: 0.2500 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 28/30\n",
      "1/1 [==============================] - 0s 166ms/step - loss: 10.0738 - acc: 0.3750 - val_loss: 11.0812 - val_acc: 0.3125\n",
      "Epoch 29/30\n",
      "1/1 [==============================] - 0s 168ms/step - loss: 12.0886 - acc: 0.2500 - val_loss: 12.0886 - val_acc: 0.2500\n",
      "Epoch 30/30\n",
      "1/1 [==============================] - 0s 168ms/step - loss: 13.0960 - acc: 0.1875 - val_loss: 12.0886 - val_acc: 0.2500\n",
      "200/200 [==============================] - 1s 3ms/step\n",
      "Test score (loss?): 9.912628555297852\n",
      "Test accuracy: 0.385\n"
     ]
    }
   ],
   "source": [
    "# MOCK\n",
    "import numpy as np\n",
    "def get_dataset() :\n",
    "    x_poly = [ [(0,0),(1,1)] for k in range(1000)]\n",
    "    y = [ k%3 for k in range(1000)]\n",
    "    return x_poly, y\n",
    "#\n",
    "def train():\n",
    "    from keras.optimizers import Adam\n",
    "    from keras.callbacks import TensorBoard, ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
    "\n",
    "    # model meta\n",
    "    input_shape = 416, 416, 3\n",
    "    #  img_size = input_shape[:2][::-1]\n",
    "    \n",
    "    # reproducible results :)\n",
    "    np.random.seed(0)\n",
    "    \n",
    "    # load dataset\n",
    "    x_poly, y = get_dataset()\n",
    "    #  input_shape = X_train[0,...].shape\n",
    "    n_classes = len(np.unique( y ))\n",
    "\n",
    "    # create train and test datasets\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    x_poly_train, x_poly_test, y_train, y_test = train_test_split( x_poly, y, test_size=0.2, random_state=0 )\n",
    "    print( 'len(x_poly_train):', len(x_poly_train) )\n",
    "    print( 'len(x_poly_test):',  len(x_poly_test) )\n",
    "    print( 'len(y_train):', len(y_train) )\n",
    "    print( 'len(y_test):',  len(y_test) )\n",
    "    \n",
    "    # create model\n",
    "    print( 'n_classes:', n_classes )\n",
    "    model = create_classifier_model( input_shape, n_classes )\n",
    "\n",
    "    # train model\n",
    "\n",
    "    # training meta\n",
    "    batch_size = 16\n",
    "    optimizer = Adam( lr=1e-4 )\n",
    "    # callbacks\n",
    "    log_dir = '/tmp/log_dir'\n",
    "    logging = TensorBoard( log_dir=log_dir )\n",
    "    reduce_lr = ReduceLROnPlateau( monitor='val_loss', factor=0.1, patience=2, verbose=1 )\n",
    "    early_stopping = EarlyStopping( monitor='val_loss', min_delta=0, patience=5, verbose=1 )\n",
    "\n",
    "    # generators\n",
    "    train_generator      = DataGenerator( x_poly_train, y_train, n_classes, batch_size )\n",
    "    validation_generator = DataGenerator( x_poly_test,  y_test,  n_classes, batch_size )\n",
    "    \n",
    "    # train\n",
    "    model.compile( loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'] )\n",
    "#     history = model.fit_generator(\n",
    "#             generator=train_generator,\n",
    "#             steps_per_epoch=max(1, num_train//batch_size),\n",
    "#             validation_data=validation_generator,\n",
    "#             validation_steps=max(1, num_val//batch_size),\n",
    "#             epochs=30,\n",
    "#             initial_epoch=0,\n",
    "#             callbacks=[logging, reduce_lr, early_stopping]\n",
    "#             )\n",
    "    history = model.fit_generator(\n",
    "            generator = train_generator,\n",
    "            steps_per_epoch=1,\n",
    "            validation_data = validation_generator,\n",
    "            validation_steps=1,\n",
    "            epochs = 30,\n",
    "            initial_epoch = 0,\n",
    "#             callbacks = [logging, reduce_lr, early_stopping]\n",
    "            )\n",
    "    # TODO: consider model.fit_generator(use_multiprocessing=True,workers=6)\n",
    "    # TODO: consider model.fit_generator() without steps_per_epoch, validation_steps\n",
    "\n",
    "    # save\n",
    "    fn = '/tmp/model.h5'\n",
    "    model.save( fn )\n",
    "\n",
    "    # eval model\n",
    "    # prepare test data for evaluation\n",
    "    X_test = [ get_img( poly, input_shape ) for poly in x_poly_test ]\n",
    "    X_test = np.stack( X_test, axis=0 )\n",
    "    from keras.utils import np_utils        \n",
    "    Y_test = np_utils.to_categorical( y_test, n_classes )\n",
    "    # eval\n",
    "    score = model.evaluate( X_test, Y_test, verbose=1 )\n",
    "    print('Test score (loss?):', score[0])\n",
    "    print('Test accuracy:', score[1])\n",
    "#\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
